{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "588effda-8731-4c83-b0f2-2fe7bbda50a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clipping Point Clouds with Grass Biomass Plot Shapefile\n",
    "# Based on Clipping Trees function for halo paper\n",
    "# PB - 02/22/2022 (lots of 2s today!)\n",
    "\n",
    "import sys\n",
    "sys.path.append('/n/home02/pbb/scripts/halo-metadata-server/')\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import laspy\n",
    "import geojson\n",
    "from shapely.geometry import Polygon\n",
    "import time\n",
    "from Classes import Collection\n",
    "from Cloud_Class import Cloud, calccover\n",
    "import time\n",
    "import concurrent.futures\n",
    "import pickle\n",
    "\n",
    "# makes matplotlib plots big\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams.update({'font.size': 14})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a28e68e-9e8d-45b2-b2ed-336e52e20e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Clipping Individual Plots/Trees\n",
    "# can change featureIDcol when working with other shapefiles\n",
    "# it should work with any shapefile with irregularly shaped polygon features, and some kind of id column\n",
    "\n",
    "# Edited 11/15/21 to work in parallel\n",
    "def iteratePolygons_parallel(feature=None, featureIDcol='Point_ID', epsg=32736):\n",
    "    \n",
    "    # Iteration time\n",
    "    itertime_start = time.time()\n",
    "\n",
    "     # subset the points to only default and ground points within the square boundary of the given shapefile feature\n",
    "    idx = [(l.x <= feature.geometry.bounds[2]) &\n",
    "           (l.x >= feature.geometry.bounds[0]) &\n",
    "           (l.y <= feature.geometry.bounds[3]) &\n",
    "           (l.y >= feature.geometry.bounds[1]) &\n",
    "           (l.points.classification != 7)] \n",
    "\n",
    "    pointset = l.points[idx]\n",
    "\n",
    "    # if points is not empty\n",
    "    if len(pointset) > 0:\n",
    "\n",
    "        # Make a geodataframe from the points\n",
    "        points_gdf = gpd.GeoDataFrame({'time':pointset.gps_time},\n",
    "                                      geometry=gpd.points_from_xy(pointset.X * l.header.x_scale + l.header.x_offset,\n",
    "                                                                  pointset.Y * l.header.y_scale + l.header.y_offset),\n",
    "                                      crs=f'EPSG:{epsg}')\n",
    "\n",
    "        # get the indices of points that intersect with the polygon feature\n",
    "        # use align=False to preserve the order of the index\n",
    "        intersects_idx = points_gdf.intersects(feature.geometry, align=False)\n",
    "\n",
    "        # NOW: subset your points again, this time, based on your intersection index\n",
    "        points_subset = pointset[intersects_idx.values]\n",
    "\n",
    "        # set the outf name and path\n",
    "        # use the featureIDcol in the shapefile to name it \n",
    "        outf = f'{outdir}/{featureIDcol}_{feature.get(featureIDcol)}.las'\n",
    "        \n",
    "                    \n",
    "        if len(points_subset) > 0:\n",
    "\n",
    "            # Skipped computing metrics for this - still need to refine the buffer zone to a 0.5m plot in CC\n",
    "            # COMPUTE METRICS:\n",
    "            # While you have your subsetted points\n",
    "            # c, p, h = calccover_parallel(points_subset)\n",
    "\n",
    "            # Iteration time end\n",
    "            itertime_end = time.time()\n",
    "            totaltime = itertime_end-itertime_start\n",
    "\n",
    "            # return points_subset, outf, totaltime, c, p, h\n",
    "            return points_subset, outf, totaltime\n",
    "\n",
    "#         # Skipped computing metrics for this - still need to refine the buffer zone to a 0.5m plot in CC\n",
    "#         try:\n",
    "            \n",
    "#             if len(points_subset) > 0:\n",
    "                \n",
    "#                 # Skipped computing metrics for this - still need to refine the buffer zone to a 0.5m plot in CC\n",
    "#                 # COMPUTE METRICS:\n",
    "#                 # While you have your subsetted points\n",
    "#                 # c, p, h = calccover_parallel(points_subset)\n",
    "\n",
    "#                 # Iteration time end\n",
    "#                 itertime_end = time.time()\n",
    "#                 totaltime = itertime_end-itertime_start\n",
    "                \n",
    "#                 # return points_subset, outf, totaltime, c, p, h\n",
    "#                 return points_subset, outf, totaltime\n",
    "        \n",
    "#         except:\n",
    "        \n",
    "#             print(f'Issue computing metrics for {featureIDcol}{feat.get(featureIDcol)}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ba61787-9557-4b20-8de1-0ddea0cbbceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # DEFINE USER INPUTS\n",
    "\n",
    "# # # \n",
    "\n",
    "# ### Before Fire\n",
    "# lasdir = '/n/davies_lab/Lab/data/processed/Africa/Kruger/Kruger_June21/LowerSabie_Fires/Before_Fire/Terrasolid_Lower_Sabie_Before_fire/output_pointcloud'\n",
    "\n",
    "# # lasinputs = [f'{lasdir}/BuffaloCamp60mCrosshatch_300mExtent_height_FOV110.las']\n",
    "# lasinputs = glob.glob(f'{lasdir}/*.las')\n",
    "\n",
    "# # # outdirectory for clipped las files\n",
    "# odir = '/n/home02/pbb/scripts/halo-metadata-server/GrassBiomass/data/out/5mBuffer'\n",
    "# # odir = '/n/home02/pbb/scripts/halo-metadata-server/GrassBiomass/data/out/50cmCenterPixel'\n",
    "\n",
    "# # outdirectory for metrics - don't need it here\n",
    "# # objoutdir = '/n/davies_lab/Users/pbb/collections/BuffCampAltitudeStudy/TreeMetrics_FOV110'\n",
    "\n",
    "# # # Designate the cshapefile to use\n",
    "# shpf = '/n/home02/pbb/scripts/halo-metadata-server/GrassBiomass/data/in/LowerSabie_GrassBiomassFieldPlots_5mBuffer/LowerSabie_GrassBiomassFieldPlots_5mBuffer.shp'\n",
    "# # shpf = '/n/home02/pbb/scripts/halo-metadata-server/GrassBiomass/data/in/LowerSabie_GrassBiomassFieldplots_50cmCenterPixel/LowerSabie_GrassBiomassFieldPlots_50cmCenterPixel.shp'\n",
    "\n",
    "# # # Feature ID col\n",
    "# featureIDcol = 'Point_ID'\n",
    "\n",
    "# # # MANUALLY ADD A PROJECT STRING (NOTE: HARDCODED FOR THIS PROJECT)\n",
    "# projstr = 'LowerSabieBeforeFire'\n",
    "\n",
    "# # #\n",
    "\n",
    "# After Fire\n",
    "# lasdir = '/n/davies_lab/Lab/data/processed/Africa/Kruger/Kruger_June21/LowerSabie_Fires/After_fire/Terrasolid/output_pointcloud'\n",
    "lasdir = '/n/davies_lab/Users/pbb/LowerSabieGrassBiomass/data/LowerSabieAfterFire'\n",
    "\n",
    "lasinputs = glob.glob(f'{lasdir}/*.las')\n",
    "\n",
    "# outdirectory for clipped las files\n",
    "odir = '/n/home02/pbb/scripts/halo-metadata-server/GrassBiomass/data/out/5mBuffer'\n",
    "# odir = '/n/home02/pbb/scripts/halo-metadata-server/GrassBiomass/data/out/50cmCenterPixel'\n",
    "\n",
    "# outdirectory for metrics - don't need it here\n",
    "# objoutdir = '/n/davies_lab/Users/pbb/collections/BuffCampAltitudeStudy/TreeMetrics_FOV110'\n",
    "\n",
    "# Designate the cshapefile to use\n",
    "shpf = '/n/home02/pbb/scripts/halo-metadata-server/GrassBiomass/data/in/LowerSabie_GrassBiomassFieldPlots_5mBuffer/LowerSabie_GrassBiomassFieldPlots_5mBuffer.shp'\n",
    "# shpf = '/n/home02/pbb/scripts/halo-metadata-server/GrassBiomass/data/in/LowerSabie_GrassBiomassFieldplots_50cmCenterPixel/LowerSabie_GrassBiomassFieldPlots_50cmCenterPixel.shp'\n",
    "\n",
    "# Feature ID col\n",
    "featureIDcol = 'Point_ID'\n",
    "\n",
    "# MANUALLY ADD A PROJECT STRING (NOTE: HARDCODED FOR THIS PROJECT)\n",
    "projstr = 'LowerSabieAfterFire'\n",
    "\n",
    "# # # \n",
    "\n",
    "# # # END USER INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4238b14e-65eb-4eb6-89cf-9754536be447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000020.las...\n",
      "\n",
      "Loaded 0.0 million points in 0.012422800064086914 seconds.\n",
      "\n",
      "Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000020.las done.\n",
      "Took 0.250974178314209 seconds to clip 98 features.\n",
      "\n",
      "Starting Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000005.las...\n",
      "\n",
      "Loaded 11.018666 million points in 0.666917085647583 seconds.\n",
      "\n",
      "Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000005.las done.\n",
      "Took 3.687946081161499 seconds to clip 98 features.\n",
      "\n",
      "Starting Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000006.las...\n",
      "\n",
      "Loaded 0.0 million points in 0.0579376220703125 seconds.\n",
      "\n",
      "Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000006.las done.\n",
      "Took 0.2851395606994629 seconds to clip 98 features.\n",
      "\n",
      "Starting Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000004.las...\n",
      "\n",
      "Loaded 21.457754 million points in 1.0556204319000244 seconds.\n",
      "\n",
      "Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000004.las done.\n",
      "Took 70.06173777580261 seconds to clip 98 features.\n",
      "\n",
      "Starting Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000017.las...\n",
      "\n",
      "Loaded 27.635991 million points in 1.7823171615600586 seconds.\n",
      "\n",
      "Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000017.las done.\n",
      "Took 67.32851839065552 seconds to clip 98 features.\n",
      "\n",
      "Starting Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000009.las...\n",
      "\n",
      "Loaded 50.487496 million points in 3.6228818893432617 seconds.\n",
      "\n",
      "Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000009.las done.\n",
      "Took 21.268161058425903 seconds to clip 98 features.\n",
      "\n",
      "Starting Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000011.las...\n",
      "\n",
      "Loaded 242.232383 million points in 13.372535228729248 seconds.\n",
      "\n",
      "Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000011.las done.\n",
      "Took 343.6463391780853 seconds to clip 98 features.\n",
      "\n",
      "Starting Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000016.las...\n",
      "\n",
      "Loaded 118.333678 million points in 9.639796495437622 seconds.\n",
      "\n",
      "Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000016.las done.\n",
      "Took 101.96464443206787 seconds to clip 98 features.\n",
      "\n",
      "Starting Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000008.las...\n",
      "\n",
      "Loaded 98.868511 million points in 5.7076616287231445 seconds.\n",
      "\n",
      "Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000008.las done.\n",
      "Took 84.80042338371277 seconds to clip 98 features.\n",
      "\n",
      "Starting Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000013.las...\n",
      "\n",
      "Loaded 243.83002 million points in 18.784719705581665 seconds.\n",
      "\n",
      "Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000013.las done.\n",
      "Took 473.1843137741089 seconds to clip 98 features.\n",
      "\n",
      "Starting Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000010.las...\n",
      "\n",
      "Loaded 82.196955 million points in 7.940303087234497 seconds.\n",
      "\n",
      "Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000010.las done.\n",
      "Took 153.77456212043762 seconds to clip 98 features.\n",
      "\n",
      "Starting Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000007.las...\n",
      "\n",
      "Loaded 241.391263 million points in 17.368202686309814 seconds.\n",
      "\n",
      "Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000007.las done.\n",
      "Took 116.77718043327332 seconds to clip 98 features.\n",
      "\n",
      "Starting Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000003.las...\n",
      "\n",
      "Loaded 0.0 million points in 0.837374210357666 seconds.\n",
      "\n",
      "Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000003.las done.\n",
      "Took 1.0668249130249023 seconds to clip 98 features.\n",
      "\n",
      "Starting Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000021.las...\n",
      "\n",
      "Loaded 0.0 million points in 0.02081608772277832 seconds.\n",
      "\n",
      "Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000021.las done.\n",
      "Took 0.25181078910827637 seconds to clip 98 features.\n",
      "\n",
      "Starting Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000014.las...\n",
      "\n",
      "Loaded 70.108382 million points in 6.787726163864136 seconds.\n",
      "\n",
      "Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000014.las done.\n",
      "Took 207.61471104621887 seconds to clip 98 features.\n",
      "\n",
      "Starting Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000002.las...\n",
      "\n",
      "Loaded 57.007422 million points in 4.151954650878906 seconds.\n",
      "\n",
      "Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000002.las done.\n",
      "Took 39.174214124679565 seconds to clip 98 features.\n",
      "\n",
      "Starting Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000018.las...\n",
      "\n",
      "Loaded 41.686403 million points in 3.7639780044555664 seconds.\n",
      "\n",
      "Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000018.las done.\n",
      "Took 17.64361047744751 seconds to clip 98 features.\n",
      "\n",
      "Starting Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000015.las...\n",
      "\n",
      "Loaded 79.138369 million points in 4.427248954772949 seconds.\n",
      "\n",
      "Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000015.las done.\n",
      "Took 122.2611632347107 seconds to clip 98 features.\n",
      "\n",
      "Starting Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000001.las...\n",
      "\n",
      "Loaded 0.0 million points in 0.24725341796875 seconds.\n",
      "\n",
      "Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000001.las done.\n",
      "Took 0.4770941734313965 seconds to clip 98 features.\n",
      "\n",
      "Starting normalized.las...\n",
      "\n",
      "Loaded 224.285702 million points in 15.961780786514282 seconds.\n",
      "\n",
      "normalized.las done.\n",
      "Took 267.7423529624939 seconds to clip 98 features.\n",
      "\n",
      "Starting Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000012.las...\n",
      "\n",
      "Loaded 63.197103 million points in 6.719210624694824 seconds.\n",
      "\n",
      "Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000012.las done.\n",
      "Took 50.82286548614502 seconds to clip 98 features.\n",
      "\n",
      "Starting Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000019.las...\n",
      "\n",
      "Loaded 0.0 million points in 0.1552107334136963 seconds.\n",
      "\n",
      "Lower_Sabie_After_fire_PointCloud_WGS84UTM36S_000019.las done.\n",
      "Took 0.38590121269226074 seconds to clip 98 features.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LOOP to clip trees from each altitude flight\n",
    "#  Uses concurrent futures to clip \n",
    "#  each polygon feature in the shapefile dataframe\n",
    "#  in parallel\n",
    "\n",
    "# Make a list of features from the shapefile\n",
    "shpdf = gpd.read_file(shpf)\n",
    "shpdf = shpdf.sort_values(featureIDcol)\n",
    "features = []\n",
    "for i, f in shpdf.iterrows():\n",
    "    features.append(f)\n",
    "\n",
    "# Subset features here \n",
    "# NOTE ONLY FOR TESTING PURPOSES - COMMENT OUT OTHERWISE\n",
    "# features = features[0:50]\n",
    "    \n",
    "# For each las file inputs\n",
    "for lasf in lasinputs:\n",
    "    \n",
    "    start_tottime = time.time()\n",
    "    \n",
    "    # HARDCODED ABOVE FOR GRASS BIOMASS PLOTS\n",
    "    # get project string for saving below\n",
    "    # projstr = lasf.split('/')[-1].split('_')[0]\n",
    "    # print(f'Starting {projstr}...\\n') \n",
    "    \n",
    "    lasfstr = lasf.split('/')[-1]\n",
    "    print(f'Starting {lasfstr}...\\n') \n",
    "    \n",
    "    # set the outdirectory for individual tree las files using the above projstr\n",
    "    outdir = f'{odir}/{projstr}/'\n",
    "    \n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    \n",
    "    start_load = time.time()\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # open the current las file\n",
    "        l = laspy.read(lasf)\n",
    "\n",
    "        end_load = time.time()\n",
    "        print(f'Loaded {l.header.point_count/1000000} million points in {end_load-start_load} seconds.\\n')\n",
    "\n",
    "        # initiate empty list of times for each project\n",
    "        times=[]\n",
    "\n",
    "        # # Initiate dictionaries for saving treeid metrics\n",
    "        # cov = {}\n",
    "        # perc = {}\n",
    "        # height = {}\n",
    "\n",
    "        # set up parallel processing for each polygon feature\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=None) as executor:\n",
    "            # return the subset of points, the outlasfile name, and the processing time for each feature\n",
    "            # Returned as a list in that order pnts_of_t = [pnts, of, time, coverdict, percdict, heightdict]\n",
    "            for pnts_of_t_c_p_h, feat in zip(executor.map(iteratePolygons_parallel, features),\n",
    "                                             features):\n",
    "\n",
    "                # # # Write output las files\n",
    "                # if there are any points to output \n",
    "                if pnts_of_t_c_p_h:\n",
    "\n",
    "                    # Record time\n",
    "                    times.append(pnts_of_t_c_p_h[2])\n",
    "                    \n",
    "                    # Store metrics in a nested dictionary\n",
    "                    # Using featureID as the key\n",
    "                    # cov[f'{featureIDcol}{feat.get(featureIDcol)}'] = pnts_of_t_c_p_h[3]\n",
    "                    # perc[f'{featureIDcol}{feat.get(featureIDcol)}'] = pnts_of_t_c_p_h[4]\n",
    "                    # height[f'{featureIDcol}{feat.get(featureIDcol)}'] = pnts_of_t_c_p_h[5]\n",
    "                    \n",
    "                    \n",
    "                    try:\n",
    "                        # if this file does not exist yet, make a new las file\n",
    "                        if not os.path.exists(pnts_of_t_c_p_h[1]):\n",
    "                            # write points to current outfile\n",
    "                            with laspy.open(pnts_of_t_c_p_h[1], mode=\"w\", header=l.header) as writer:\n",
    "                                writer.write_points(pnts_of_t_c_p_h[0])\n",
    "                        # else: append to the current las file\n",
    "                        else:\n",
    "                            # write points to current outfile\n",
    "                            with laspy.open(pnts_of_t_c_p_h[1], mode=\"a\", header=l.header) as writer:\n",
    "                                writer.append_points(pnts_of_t_c_p_h[0])\n",
    "                    except: \n",
    "                        \n",
    "                        print(f'Unable to write lasfile: {pnts_of_t_c_p_h[1]}')\n",
    "                                \n",
    "        \n",
    "        lasfstr = lasf.split('/')[-1]\n",
    "        end_tottime = time.time()\n",
    "        print(f'{lasfstr} done.\\nTook {end_tottime - start_tottime} seconds to clip {len(features)} features.\\n')\n",
    "                              \n",
    "    except ValueError as ve:\n",
    "        \n",
    "        lasfstr = lasf.split('/')[-1]\n",
    "        print(f'Could not open {lasfstr} \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76d8d033-a4b3-405f-8f16-94606b2347c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to automatically add plotid with laspy\n",
    "# ran into issues writing the file header\n",
    "# las = laspy.read(lasf)\n",
    "# plotid = pnts_of_t_c_p_h[1].split('ID_')[-1].split('.')[0]\n",
    "# las.plot = np.repeat(plotid, las.header.point_count).astype(np.uint16)\n",
    "# las.add_extra_dim(laspy.ExtraBytesParams(name=\"plot\", type=np.uint16))\n",
    "\n",
    "# for i in las.point_format.dimension_names:\n",
    "#     print(i)\n",
    "\n",
    "# # write points to current outfile\n",
    "# with laspy.open('test.las', mode=\"w\", header=l.header) as writer:\n",
    "#     writer.write_points(las)\n",
    "    \n",
    "# las.header.point_format.dimensions\n",
    "# # \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-Halo]",
   "language": "python",
   "name": "conda-env-.conda-Halo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
